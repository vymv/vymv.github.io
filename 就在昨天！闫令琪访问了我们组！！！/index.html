<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="就在昨天！闫令琪访问了我们组！！！"><meta name="keywords" content="离线渲染,实时渲染,前沿论文"><meta name="author" content="vymv"><meta name="copyright" content="vymv"><title>就在昨天！闫令琪访问了我们组！！！ | vymv's Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.3.0'
} </script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="vymv's Blog" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Appearance"><span class="toc-number">1.</span> <span class="toc-text">Appearance</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SpongeCake"><span class="toc-number">1.1.</span> <span class="toc-text">SpongeCake</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Yarn-based-Cloth"><span class="toc-number">1.2.</span> <span class="toc-text">Yarn-based Cloth</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A2%E5%8A%A8%E5%85%89%E5%AD%A6"><span class="toc-number">2.</span> <span class="toc-text">波动光学</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Performance"><span class="toc-number">3.</span> <span class="toc-text">Performance</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Aggregation-of-Fur-Appearance"><span class="toc-number">3.1.</span> <span class="toc-text">Aggregation of Fur Appearance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%BD%93%E7%B4%A0%E5%8C%96"><span class="toc-number">3.2.</span> <span class="toc-text">场景体素化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E5%B1%82%E6%9D%90%E8%B4%A8"><span class="toc-number">4.</span> <span class="toc-text">神经网络分层材质</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E5%87%BD%E6%95%B0"><span class="toc-number">5.</span> <span class="toc-text">基函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%92%E5%B8%A7"><span class="toc-number">6.</span> <span class="toc-text">插帧</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ROMA"><span class="toc-number">7.</span> <span class="toc-text">ROMA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Open-problem"><span class="toc-number">8.</span> <span class="toc-text">Open problem</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E7%A2%8E%E7%A2%8E%E5%BF%B5"><span class="toc-number">9.</span> <span class="toc-text">一些碎碎念</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/images/avatar.jpg"></div><div class="author-info__name text-center">vymv</div><div class="author-info__description text-center">A CS student studing at NJU</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">7</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">7</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">2</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/images/topimage7.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">vymv's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">就在昨天！闫令琪访问了我们组！！！</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-06-20</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>闫令琪老师6.19报告的笔记和感想。</p>
<h2 id="Appearance"><a href="#Appearance" class="headerlink" title="Appearance"></a><strong>Appearance</strong></h2><ul>
<li>外观是物体的本质属性。给定一个外观，我们就可以知道，从给定的光照，不同的角度，物体看上去长什么样子。</li>
<li>外观可以让我们在不同的条件下，看到这个物体长什么样。比如brdf。</li>
<li>外观建模为什么难，因为三高，“高维”、“高频”，“高秩”。</li>
<li>高维：例如brdf，输入方向，输出方向，就4维了，再加上各处不同就是6维了。</li>
<li>高频：“看这张图，如果我的光源稍微移动一点，或者镜头稍微移动一点，你会发现上面有很多闪烁的小颗粒，那么它的颜色在时间的变化上是一个非常剧烈而高频的信号”。（这里没太听懂）</li>
<li>高秩：“看这块布料，如果我们忽略它们内部的变化，其实就是一些重复横竖的织物，用矩阵可以表示为0101……，那我们知道这应该是一个低秩的矩阵。但是实际上是，布料的每一处都有自己独特的细节，很难找到两块完全一样的。如果你忽略了这些变化，得到的可能就是整板一块的结果，就像phong模型得到的结果不好看”。</li>
</ul>
<p><strong>Appearance是不是2D贴图</strong></p>
<p>比如孔雀的尾巴，鸽子的脖子，光盘在不同方向看的彩色，波动光学的一些效应，这些都显然不是2D贴图。</p>
<p><strong>Nerf是不是Appearance</strong></p>
<p>一定程度上，它描述的比之前2D贴图要多，但是它并不完备。Nerf其实就相当于多张照片，一张照片不足以描述Appearance，但是多张照片也只是更多而已。</p>
<p>可以允许不同方向的观察，但是并不能做比如大家现在试图研究的Relighting，或者把材质换一换。</p>
<h3 id="SpongeCake"><a href="#SpongeCake" class="headerlink" title="SpongeCake"></a>SpongeCake</h3><ul>
<li>结构决定性质，对外观的建模，实际上也就是对影响外观的结构，进行建模。</li>
<li>例如我们的<a target="_blank" rel="noopener" href="http://sites.cs.ucsb.edu/~lingqi/publications/paper_spongecake.pdf">Spongcake模型</a>。有各种描述物体反射属性的模型，比如phong描述起来像塑料，微表面能够描述金属非金属玻璃表面磨砂水，有没有一种模型可以描述更多的外观。</li>
<li>我们就做了一个出来。<ul>
<li>它可以描述微表面模型</li>
<li>可以描述多层材质，大家可以看面前的桌子椅子，这些都是木头，为什么上面有高光呢，因为上面刷了层漆，至少是两层的材质，但是两层的材质不是简单的两层brdf加起来。</li>
<li>可以描述sheen，从grazing angle看表面，会有比较强的lobe，广泛见于衣服上，尤其是人们背光的时候，会感觉人们衣服的边缘在发光。</li>
</ul>
</li>
</ul>
<p>看这幅图，我们的模型做的窗帘，当外面光照强烈时，光可以穿过窗帘，发生透射现象，当夜晚室内比较亮时，我们又可以观察到窗帘的反射现象。</p>
<p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image-1687365736696.png" alt="img"></p>
<p>所以回到spongecake的名字，我们的材质就像海绵蛋糕一样，每一层我们可以用一个microflake模型，也类似微表面，层与层之间我们就不考虑折射了。</p>
<p><strong>弹射一次</strong></p>
<ul>
<li>给定光线的入射和出射位置，先考虑光线在模型里弹射一次，我们可以假定一个反射的深度，然后对深度积分。</li>
</ul>
<p><strong>弹射多次</strong></p>
<ul>
<li>和弹射一次相比，其实弹射多次看上去差不多，但是会稍微更明亮一些，所以我们可以用单次弹射近似多次弹射。</li>
<li>用神经网络可以fit，输入单次弹射的参数，输出多次弹射的结果。（如果只有两个参数，那用纹理足够表示，如果有多个参数，需要一张多维的表，神经网络非常适合保存和压缩高维表格）</li>
</ul>
<h3 id="Yarn-based-Cloth"><a href="#Yarn-based-Cloth" class="headerlink" title="Yarn-based Cloth"></a><strong>Yarn-based Cloth</strong></h3><ul>
<li>今年有一篇刚刚被接收的，用spongecake模型做布料的模型。</li>
<li>fiber缠绕变成plies（股），plies缠绕形成yarn（线），yarn纺织到一块，由于摩擦力，形成一块可靠的布。</li>
<li>最简单的方法是把一根根fiber全部渲染出来。自然开销很大，但结果很真实。</li>
<li>也有人把场景划分成格子，每个格子里很多fiber，研究一个格子里的光学属性，相对简化一些。</li>
<li>实时渲染中，更想考虑的是surface based，但是看起来太不真实。</li>
<li>我们的模型只需要定义四张图，mask（线中间有缝）、法线、tangent（朝向）、高度场（低的地方容易暗一些）</li>
</ul>
<p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image-1687365736881.png" alt="img"></p>
<ul>
<li>看了视频，细节保留很到位，但是镜头运动起来还是有一些噪点。</li>
</ul>
<h2 id="波动光学"><a href="#波动光学" class="headerlink" title="波动光学"></a>波动光学</h2><ul>
<li>我们现在的reference都是基于所谓的渲染方程。</li>
<li>描述的是几何光学，物体在波动光学下就不对了</li>
<li>光的衍射：当障碍物很小的时候光线可能会绕过去（不一定沿直线传播）</li>
<li>光的干涉：假设光线和光线之间没有相互作用。</li>
<li>光路的可逆性：在波动光学下也不对。</li>
</ul>
<p>有没有办法看看真的ground truth是什么样的。</p>
<p>这一系列工作很难，不建议大家读。</p>
<p>我们研究了波动光学下，光线的发生、光线的传播、材质的属性、光线如何和材质作用、光线最后如何呈像。</p>
<p>仍然是光线追踪的思路，在光线追踪的同时多记录一点信息。</p>
<p>不同的光源，发出的光线，相干性都不一样，和物体作用之后相干性又会发生变化。</p>
<p>这个工作相当于渲染方程的超集，做了若干假设之后，可以退化到渲染方程。</p>
<p>速度上，还是光线追踪相关，没有比几何光学贵到一个数量级以上，自然而然可以放到实时渲染里面。</p>
<p>和nvidia合作，把波动光学相关拿到他们的实时渲染框架中去。（看上去和几何光学差别不大）</p>
<h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><ul>
<li>我们想得到真实的外观，通常都会伴随着复杂的几何</li>
<li>如果几何的复杂度是n，那么渲染的复杂度至少是O(n)的层级</li>
<li>要做到，简化几何的同时，渲染的结果不发生变化</li>
<li>几何的简化，必然需要外观建模的复杂化</li>
<li>nanite动态的做层次结构的选择，预先计算好场景的简化，只做几何的简化，并没有额外处理appearance，但是山洞是diffuse的，所有看起来正确。</li>
</ul>
<h3 id="Aggregation-of-Fur-Appearance"><a href="#Aggregation-of-Fur-Appearance" class="headerlink" title="Aggregation of Fur Appearance"></a><strong>Aggregation of Fur Appearance</strong></h3><p><a target="_blank" rel="noopener" href="http://sites.cs.ucsb.edu/~lingqi/project_page/fur_aggregation/index.html">http://sites.cs.ucsb.edu/~lingqi/project_page/fur_aggregation/index.html</a></p>
<ul>
<li>一只小老鼠150w根毛发，开销巨大</li>
<li>离的远的，就少建模几根，例如1.5w根，现在的一根代表原来的100根。</li>
</ul>
<p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image-1687365736705.png" alt="img"></p>
<ul>
<li>如左下右下，显得非常生硬。</li>
<li>这是因为只简化了几何，不对外观复杂化，100根的毛发，还用原来1根的光照模型。</li>
<li>我们的工作是，给定n根毛发，给定绑定密度，每一根的属性，输出它们合在一块的外观。</li>
</ul>
<p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image.png" alt="img"></p>
<ul>
<li>多根绑在一起，就好像单根变暗了一些，整体看起来更亮一些。</li>
<li>给一根毛发的属性，给一些统计学数据，输出appearance，本质上还是巨大的表格，高维的表，还是拿神经网络最合适。</li>
<li>结果和gt真的几乎看不出差别。</li>
</ul>
<h3 id="场景体素化"><a href="#场景体素化" class="headerlink" title="场景体素化"></a>场景体素化</h3><ul>
<li>为什么要体素化来渲染<ul>
<li>可以自由选择层级，用多大的格子。</li>
</ul>
</li>
<li>一个格子内部不做区分了，原本的几何被丢弃了</li>
<li>格子内可能有实际的物体表面，格子内部可能存在各项异性的问题，从某些方向打光，格子不透光，某些则全透光</li>
<li>多个表面，在一个格子中的correlation也需要考虑，如果两个表面并排挡住了50%，那整体挡住了50%，如果各自挡住了50%，则整体挡住了100%。</li>
<li>最左边这幅图，可以看从上往下数第四根光线，理论上交点应该在格子A2，但实际中间这幅图 ，却是和B1相交的，为了解决这个问题，我们使用了如右图的椭球模型</li>
</ul>
<p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image.png" alt="img"></p>
<ul>
<li>推出一个格子aggregate之后，整个的brdf是什么样的。</li>
<li>典型的好处是我们可以做非常specular的物体。</li>
<li>result几乎看不出来是volume描述的，比如大吊灯，菲尔铁塔。</li>
<li>树、花，因为格子里树叶的排布非常随机，corelation非常低，表示起来相对容易。</li>
<li>结构上看起来越是大面积的表面，处理起来越是困难。</li>
<li>这篇文章还没在闫佬的主页上？</li>
</ul>
<h2 id="神经网络分层材质"><a href="#神经网络分层材质" class="headerlink" title="神经网络分层材质"></a>神经网络分层材质</h2><p><a target="_blank" rel="noopener" href="https://wangningbei.github.io/2022/NLBRDF.html">Neural Layered BRDFs</a></p>
<ul>
<li>神经网络非常适合做压缩记录一个高维表格，神经渲染的工作，相当于用神经网络做一些端到端的事情，这些事情对神经网络负担较大。</li>
<li>可以用神经网络做一些小的事情。</li>
<li>brdf也是一个高维数据块，有无数方法，可以把brdf压缩成latent</li>
<li>两个brdf都已经被压缩，如果layer成一个brdf，会是什么样的表示呢</li>
<li>通常是把压缩的brdf展开，进行光线追踪，再用神经网络压缩</li>
<li>brdf和brdf之间的运算，可以当做操作算子，这个算子用神经网络表示</li>
<li>用神经网络简化了计算过程</li>
</ul>
<h2 id="基函数"><a href="#基函数" class="headerlink" title="基函数"></a>基函数</h2><p><a target="_blank" rel="noopener" href="https://starry316.github.io/sigas2022/index.html">Lightweight Neural Basis Functions for All-Frequency Shading</a></p>
<ul>
<li>小波不支持旋转。大家通常认为小波很擅长描述高频的东西，但小波每一个rgb通道单独运作，每一个通道都有自己产生的问题，三个通道都会产生一些偏差。压缩能力非常糟糕。</li>
<li>SH不擅长描述高频</li>
<li>我们设计了一种基函数</li>
<li>但渲染过程中需要逐线程的跑神经网络，渲染过程会比较慢</li>
</ul>
<h2 id="插帧"><a href="#插帧" class="headerlink" title="插帧"></a>插帧</h2><ul>
<li>这个工作是和我们组合作的，2021年投了<a target="_blank" rel="noopener" href="http://sites.cs.ucsb.edu/~lingqi/publications/paper_extranet.pdf">sig</a></li>
<li>supersampling（放大分辨率，DLSS）</li>
<li>interpolation（在时间轴上生成更多的帧，Extranet）</li>
<li>interpolation需要前一帧和后一帧来插值中间一帧，这种做法会导致延迟，因为得有下一帧，才能生成当前这帧。（nvidia有项工作叫reflex，可以降低其他工作产生的延迟，可以理解为拆东墙补西墙，仍然用了interpolation）</li>
<li>而extrapolation则不会。我们无法预知下一帧用户的动作是什么样的，我们可以先拿下一帧的gbuffer，虽然还没有渲染下一帧，但是知道这些物体该如何移动了，把大概的框架交给神经网络预测。</li>
<li>今年投稿的工作ExtraSS，在时间和空间上都放大了（我们组也同时在研究，也跟闫佬投了同期的siggraph，但效果实在是太差了）</li>
</ul>
<h2 id="ROMA"><a href="#ROMA" class="headerlink" title="ROMA"></a>ROMA</h2><p><a target="_blank" rel="noopener" href="https://zheng95z.github.io/publications/roma23">Ray-aligned Occupancy Map Array for Fast Approximate Ray Tracing 快速近似光线追踪</a>。</p>
<ul>
<li>SDF的Trace并不快，而且SDF的建立非常非常慢</li>
<li>提出Roma，希望建立起来非常快，trace起来也非常快</li>
<li>UE里物体如果运动，globalSDF得重新建立</li>
<li>举了一个例子，建立距离场需要花3ms，我们只需要1ms</li>
<li>我们的Tracing速度和Hardware ray tracing相当</li>
<li>Occupancy map在工业界叫bit brick，光栅化后如果格子里有物体就是1，没物体就是0。</li>
<li>Occupancy map直接trace没有很快，得做很多次迭代，有一种情况下不用迭代：当光线正好沿着一个纹素的纵深方向去的时候。</li>
<li>这是一个low bit算法，给一个二进制数字串，从最低位开始数，第一个1出现在第几位，可以通过位运算，在O(1)的时间拿到。</li>
<li>但不是所有光线都是沿着纵深方向走的，如果角度差的很小，我们可以进行近似。</li>
<li>为了任何一个查询方向和纵深方向都很近，做很多个不同的旋转方向，很快。</li>
<li>但是因为近似过多，噪点很多，过一个denoiser，把错误分摊到时间上，效果很好。</li>
<li>如果不愿意近似，也可以在最近的方向上做trace，因为方向很近，迭代的步数也不会很高（5~8次）。</li>
<li>Roma wasn’t built in a day, but within 1ms</li>
</ul>
<h2 id="Open-problem"><a href="#Open-problem" class="headerlink" title="Open problem"></a>Open problem</h2><ul>
<li>相比神经渲染，神经网络在渲染领域里面的应用，更合适的说法是nerual aidded rendering，不太希望神经网络能替代整个渲染过程。</li>
<li>Apearance应该是8维的函数。bssrdf。Nerf是4维的出射光场，所以不是Appearance。</li>
<li>任何和人类相关的工作都很难，眼镜，皮肤非常复杂，试图挑战一下这些困难，不清楚数据驱动方法好，还是建模方法好，尤其是皮肤数据，在国外是绝对不可能有的。</li>
<li>动画会放大恐怖谷效应，可以研究photorealistic animation。</li>
</ul>
<h2 id="一些碎碎念"><a href="#一些碎碎念" class="headerlink" title="一些碎碎念"></a>一些碎碎念</h2><ul>
<li>闫老师人真的超随和！！！好多外地的同学早上赶高铁来南京，有上海的，安徽的，武汉的。好多人抱着虎书找他要签名，还有人没带纸，直接让闫佬签在电脑上hhh</li>
<li>虽然时间仓促没来得及多交流，感觉我们组的同学有机会和闫佬私下接触真的很有幸了！！！</li>
<li>对于神经渲染会不会取代传统的渲染，闫老师说，首先它得承担现在传统渲染能做的一些事情，例如场景能支持编辑，能分离不同的现象，这些目前还是走传统的渲染管线，如果这条路走了，可能也和现在的渲染管线遇到的问题一样了，这是一个难题。而且目前神经渲染实在是太慢了。nerf会不会在爬恐怖谷的时候也开始往下掉，这也是有可能的。</li>
<li>偷偷问了闫佬，原本定在今年的离线课程咕咕了，可能得明年了。</li>
<li>问了闫老师对实时全局光照的看法，目前科研界的研究可能还是太离线了，少有真正落地的项目，他也在考虑如何解决，可能要考虑做一个全新的算法出来，但没有展开太多（可能是后面排队要签名的人太多了。。。</li>
</ul>
<p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image-20230622004457254.png" alt="image-20230622004457254"></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">vymv</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://vymv.github.io/就在昨天！闫令琪访问了我们组！！！/">https://vymv.github.io/就在昨天！闫令琪访问了我们组！！！/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://vymv.github.io">vymv's Blog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A6%BB%E7%BA%BF%E6%B8%B2%E6%9F%93/">离线渲染</a><a class="post-meta__tags" href="/tags/%E5%AE%9E%E6%97%B6%E6%B8%B2%E6%9F%93/">实时渲染</a><a class="post-meta__tags" href="/tags/%E5%89%8D%E6%B2%BF%E8%AE%BA%E6%96%87/">前沿论文</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/%E7%94%A8%E4%B8%80%E4%B8%AA%E5%91%A8%E6%9C%AB%E8%AE%AD%E7%BB%83%E4%BA%86gls%E7%9A%84AI%E6%AD%8C%E6%89%8B/"><i class="fa fa-chevron-left">  </i><span>用一个周末训练了一个AI歌手</span></a></div><div class="next-post pull-right"><a href="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/"><span>StableDiffusion初尝试和探索</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(/images/topimage7.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2024 By vymv</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>
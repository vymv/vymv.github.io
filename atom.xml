<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>vymv&#39;s Blog</title>
  
  <subtitle>Curious About The World</subtitle>
  <link href="https://vymv.github.io/atom.xml" rel="self"/>
  
  <link href="https://vymv.github.io/"/>
  <updated>2023-07-23T09:59:23.319Z</updated>
  <id>https://vymv.github.io/</id>
  
  <author>
    <name>vymv</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>就在昨天！闫令琪访问了我们组！！！</title>
    <link href="https://vymv.github.io/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/"/>
    <id>https://vymv.github.io/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/</id>
    <published>2023-06-19T16:36:36.000Z</published>
    <updated>2023-07-23T09:59:23.319Z</updated>
    
    <content type="html"><![CDATA[<p>闫令琪老师6.19报告的笔记和感想。</p><h2 id="Appearance"><a href="#Appearance" class="headerlink" title="Appearance"></a><strong>Appearance</strong></h2><ul><li>外观是物体的本质属性。给定一个外观，我们就可以知道，从给定的光照，不同的角度，物体看上去长什么样子。</li><li>外观可以让我们在不同的条件下，看到这个物体长什么样。比如brdf。</li><li>外观建模为什么难，因为三高，“高维”、“高频”，“高秩”。</li><li>高维：例如brdf，输入方向，输出方向，就4维了，再加上各处不同就是6维了。</li><li>高频：“看这张图，如果我的光源稍微移动一点，或者镜头稍微移动一点，你会发现上面有很多闪烁的小颗粒，那么它的颜色在时间的变化上是一个非常剧烈而高频的信号”。（这里没太听懂）</li><li>高秩：“看这块布料，如果我们忽略它们内部的变化，其实就是一些重复横竖的织物，用矩阵可以表示为0101……，那我们知道这应该是一个低秩的矩阵。但是实际上是，布料的每一处都有自己独特的细节，很难找到两块完全一样的。如果你忽略了这些变化，得到的可能就是整板一块的结果，就像phong模型得到的结果不好看”。</li></ul><p><strong>Appearance是不是2D贴图</strong></p><p>比如孔雀的尾巴，鸽子的脖子，光盘在不同方向看的彩色，波动光学的一些效应，这些都显然不是2D贴图。</p><p><strong>Nerf是不是Appearance</strong></p><p>一定程度上，它描述的比之前2D贴图要多，但是它并不完备。Nerf其实就相当于多张照片，一张照片不足以描述Appearance，但是多张照片也只是更多而已。</p><p>可以允许不同方向的观察，但是并不能做比如大家现在试图研究的Relighting，或者把材质换一换。</p><h3 id="SpongeCake"><a href="#SpongeCake" class="headerlink" title="SpongeCake"></a>SpongeCake</h3><ul><li>结构决定性质，对外观的建模，实际上也就是对影响外观的结构，进行建模。</li><li>例如我们的<a href="http://sites.cs.ucsb.edu/~lingqi/publications/paper_spongecake.pdf">Spongcake模型</a>。有各种描述物体反射属性的模型，比如phong描述起来像塑料，微表面能够描述金属非金属玻璃表面磨砂水，有没有一种模型可以描述更多的外观。</li><li>我们就做了一个出来。<ul><li>它可以描述微表面模型</li><li>可以描述多层材质，大家可以看面前的桌子椅子，这些都是木头，为什么上面有高光呢，因为上面刷了层漆，至少是两层的材质，但是两层的材质不是简单的两层brdf加起来。</li><li>可以描述sheen，从grazing angle看表面，会有比较强的lobe，广泛见于衣服上，尤其是人们背光的时候，会感觉人们衣服的边缘在发光。</li></ul></li></ul><p>看这幅图，我们的模型做的窗帘，当外面光照强烈时，光可以穿过窗帘，发生透射现象，当夜晚室内比较亮时，我们又可以观察到窗帘的反射现象。</p><p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image-1687365736696.png" alt="img"></p><p>所以回到spongecake的名字，我们的材质就像海绵蛋糕一样，每一层我们可以用一个microflake模型，也类似微表面，层与层之间我们就不考虑折射了。</p><p><strong>弹射一次</strong></p><ul><li>给定光线的入射和出射位置，先考虑光线在模型里弹射一次，我们可以假定一个反射的深度，然后对深度积分。</li></ul><p><strong>弹射多次</strong></p><ul><li>和弹射一次相比，其实弹射多次看上去差不多，但是会稍微更明亮一些，所以我们可以用单次弹射近似多次弹射。</li><li>用神经网络可以fit，输入单次弹射的参数，输出多次弹射的结果。（如果只有两个参数，那用纹理足够表示，如果有多个参数，需要一张多维的表，神经网络非常适合保存和压缩高维表格）</li></ul><h3 id="Yarn-based-Cloth"><a href="#Yarn-based-Cloth" class="headerlink" title="Yarn-based Cloth"></a><strong>Yarn-based Cloth</strong></h3><ul><li>今年有一篇刚刚被接收的，用spongecake模型做布料的模型。</li><li>fiber缠绕变成plies（股），plies缠绕形成yarn（线），yarn纺织到一块，由于摩擦力，形成一块可靠的布。</li><li>最简单的方法是把一根根fiber全部渲染出来。自然开销很大，但结果很真实。</li><li>也有人把场景划分成格子，每个格子里很多fiber，研究一个格子里的光学属性，相对简化一些。</li><li>实时渲染中，更想考虑的是surface based，但是看起来太不真实。</li><li>我们的模型只需要定义四张图，mask（线中间有缝）、法线、tangent（朝向）、高度场（低的地方容易暗一些）</li></ul><p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image-1687365736881.png" alt="img"></p><ul><li>看了视频，细节保留很到位，但是镜头运动起来还是有一些噪点。</li></ul><h2 id="波动光学"><a href="#波动光学" class="headerlink" title="波动光学"></a>波动光学</h2><ul><li>我们现在的reference都是基于所谓的渲染方程。</li><li>描述的是几何光学，物体在波动光学下就不对了</li><li>光的衍射：当障碍物很小的时候光线可能会绕过去（不一定沿直线传播）</li><li>光的干涉：假设光线和光线之间没有相互作用。</li><li>光路的可逆性：在波动光学下也不对。</li></ul><p>有没有办法看看真的ground truth是什么样的。</p><p>这一系列工作很难，不建议大家读。</p><p>我们研究了波动光学下，光线的发生、光线的传播、材质的属性、光线如何和材质作用、光线最后如何呈像。</p><p>仍然是光线追踪的思路，在光线追踪的同时多记录一点信息。</p><p>不同的光源，发出的光线，相干性都不一样，和物体作用之后相干性又会发生变化。</p><p>这个工作相当于渲染方程的超集，做了若干假设之后，可以退化到渲染方程。</p><p>速度上，还是光线追踪相关，没有比几何光学贵到一个数量级以上，自然而然可以放到实时渲染里面。</p><p>和nvidia合作，把波动光学相关拿到他们的实时渲染框架中去。（看上去和几何光学差别不大）</p><h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><ul><li>我们想得到真实的外观，通常都会伴随着复杂的几何</li><li>如果几何的复杂度是n，那么渲染的复杂度至少是O(n)的层级</li><li>要做到，简化几何的同时，渲染的结果不发生变化</li><li>几何的简化，必然需要外观建模的复杂化</li><li>nanite动态的做层次结构的选择，预先计算好场景的简化，只做几何的简化，并没有额外处理appearance，但是山洞是diffuse的，所有看起来正确。</li></ul><h3 id="Aggregation-of-Fur-Appearance"><a href="#Aggregation-of-Fur-Appearance" class="headerlink" title="Aggregation of Fur Appearance"></a><strong>Aggregation of Fur Appearance</strong></h3><p><a href="http://sites.cs.ucsb.edu/~lingqi/project_page/fur_aggregation/index.html">http://sites.cs.ucsb.edu/~lingqi/project_page/fur_aggregation/index.html</a></p><ul><li>一只小老鼠150w根毛发，开销巨大</li><li>离的远的，就少建模几根，例如1.5w根，现在的一根代表原来的100根。</li></ul><p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image-1687365736705.png" alt="img"></p><ul><li>如左下右下，显得非常生硬。</li><li>这是因为只简化了几何，不对外观复杂化，100根的毛发，还用原来1根的光照模型。</li><li>我们的工作是，给定n根毛发，给定绑定密度，每一根的属性，输出它们合在一块的外观。</li></ul><p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image.png" alt="img"></p><ul><li>多根绑在一起，就好像单根变暗了一些，整体看起来更亮一些。</li><li>给一根毛发的属性，给一些统计学数据，输出appearance，本质上还是巨大的表格，高维的表，还是拿神经网络最合适。</li><li>结果和gt真的几乎看不出差别。</li></ul><h3 id="场景体素化"><a href="#场景体素化" class="headerlink" title="场景体素化"></a>场景体素化</h3><ul><li>为什么要体素化来渲染<ul><li>可以自由选择层级，用多大的格子。</li></ul></li><li>一个格子内部不做区分了，原本的几何被丢弃了</li><li>格子内可能有实际的物体表面，格子内部可能存在各项异性的问题，从某些方向打光，格子不透光，某些则全透光</li><li>多个表面，在一个格子中的correlation也需要考虑，如果两个表面并排挡住了50%，那整体挡住了50%，如果各自挡住了50%，则整体挡住了100%。</li><li>最左边这幅图，可以看从上往下数第四根光线，理论上交点应该在格子A2，但实际中间这幅图 ，却是和B1相交的，为了解决这个问题，我们使用了如右图的椭球模型</li></ul><p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image.png" alt="img"></p><ul><li>推出一个格子aggregate之后，整个的brdf是什么样的。</li><li>典型的好处是我们可以做非常specular的物体。</li><li>result几乎看不出来是volume描述的，比如大吊灯，菲尔铁塔。</li><li>树、花，因为格子里树叶的排布非常随机，corelation非常低，表示起来相对容易。</li><li>结构上看起来越是大面积的表面，处理起来越是困难。</li><li>这篇文章还没在闫佬的主页上？</li></ul><h2 id="神经网络分层材质"><a href="#神经网络分层材质" class="headerlink" title="神经网络分层材质"></a>神经网络分层材质</h2><p><a href="https://wangningbei.github.io/2022/NLBRDF.html">Neural Layered BRDFs</a></p><ul><li>神经网络非常适合做压缩记录一个高维表格，神经渲染的工作，相当于用神经网络做一些端到端的事情，这些事情对神经网络负担较大。</li><li>可以用神经网络做一些小的事情。</li><li>brdf也是一个高维数据块，有无数方法，可以把brdf压缩成latent</li><li>两个brdf都已经被压缩，如果layer成一个brdf，会是什么样的表示呢</li><li>通常是把压缩的brdf展开，进行光线追踪，再用神经网络压缩</li><li>brdf和brdf之间的运算，可以当做操作算子，这个算子用神经网络表示</li><li>用神经网络简化了计算过程</li></ul><h2 id="基函数"><a href="#基函数" class="headerlink" title="基函数"></a>基函数</h2><p><a href="https://starry316.github.io/sigas2022/index.html">Lightweight Neural Basis Functions for All-Frequency Shading</a></p><ul><li>小波不支持旋转。大家通常认为小波很擅长描述高频的东西，但小波每一个rgb通道单独运作，每一个通道都有自己产生的问题，三个通道都会产生一些偏差。压缩能力非常糟糕。</li><li>SH不擅长描述高频</li><li>我们设计了一种基函数</li><li>但渲染过程中需要逐线程的跑神经网络，渲染过程会比较慢</li></ul><h2 id="插帧"><a href="#插帧" class="headerlink" title="插帧"></a>插帧</h2><ul><li>这个工作是和我们组合作的，2021年投了<a href="http://sites.cs.ucsb.edu/~lingqi/publications/paper_extranet.pdf">sig</a></li><li>supersampling（放大分辨率，DLSS）</li><li>interpolation（在时间轴上生成更多的帧，Extranet）</li><li>interpolation需要前一帧和后一帧来插值中间一帧，这种做法会导致延迟，因为得有下一帧，才能生成当前这帧。（nvidia有项工作叫reflex，可以降低其他工作产生的延迟，可以理解为拆东墙补西墙，仍然用了interpolation）</li><li>而extrapolation则不会。我们无法预知下一帧用户的动作是什么样的，我们可以先拿下一帧的gbuffer，虽然还没有渲染下一帧，但是知道这些物体该如何移动了，把大概的框架交给神经网络预测。</li><li>今年投稿的工作ExtraSS，在时间和空间上都放大了（我们组也同时在研究，也跟闫佬投了同期的siggraph，但效果实在是太差了）</li></ul><h2 id="ROMA"><a href="#ROMA" class="headerlink" title="ROMA"></a>ROMA</h2><p><a href="https://zheng95z.github.io/publications/roma23">Ray-aligned Occupancy Map Array for Fast Approximate Ray Tracing 快速近似光线追踪</a>。</p><ul><li>SDF的Trace并不快，而且SDF的建立非常非常慢</li><li>提出Roma，希望建立起来非常快，trace起来也非常快</li><li>UE里物体如果运动，globalSDF得重新建立</li><li>举了一个例子，建立距离场需要花3ms，我们只需要1ms</li><li>我们的Tracing速度和Hardware ray tracing相当</li><li>Occupancy map在工业界叫bit brick，光栅化后如果格子里有物体就是1，没物体就是0。</li><li>Occupancy map直接trace没有很快，得做很多次迭代，有一种情况下不用迭代：当光线正好沿着一个纹素的纵深方向去的时候。</li><li>这是一个low bit算法，给一个二进制数字串，从最低位开始数，第一个1出现在第几位，可以通过位运算，在O(1)的时间拿到。</li><li>但不是所有光线都是沿着纵深方向走的，如果角度差的很小，我们可以进行近似。</li><li>为了任何一个查询方向和纵深方向都很近，做很多个不同的旋转方向，很快。</li><li>但是因为近似过多，噪点很多，过一个denoiser，把错误分摊到时间上，效果很好。</li><li>如果不愿意近似，也可以在最近的方向上做trace，因为方向很近，迭代的步数也不会很高（5~8次）。</li><li>Roma wasn’t built in a day, but within 1ms</li></ul><h2 id="Open-problem"><a href="#Open-problem" class="headerlink" title="Open problem"></a>Open problem</h2><ul><li>相比神经渲染，神经网络在渲染领域里面的应用，更合适的说法是nerual aidded rendering，不太希望神经网络能替代整个渲染过程。</li><li>Apearance应该是8维的函数。bssrdf。Nerf是4维的出射光场，所以不是Appearance。</li><li>任何和人类相关的工作都很难，眼镜，皮肤非常复杂，试图挑战一下这些困难，不清楚数据驱动方法好，还是建模方法好，尤其是皮肤数据，在国外是绝对不可能有的。</li><li>动画会放大恐怖谷效应，可以研究photorealistic animation。</li></ul><h2 id="一些碎碎念"><a href="#一些碎碎念" class="headerlink" title="一些碎碎念"></a>一些碎碎念</h2><ul><li>闫老师人真的超随和！！！好多外地的同学早上赶高铁来南京，有上海的，安徽的，武汉的。好多人抱着虎书找他要签名，还有人没带纸，直接让闫佬签在电脑上hhh</li><li>虽然时间仓促没来得及多交流，感觉我们组的同学有机会和闫佬私下接触真的很有幸了！！！</li><li>对于神经渲染会不会取代传统的渲染，闫老师说，首先它得承担现在传统渲染能做的一些事情，例如场景能支持编辑，能分离不同的现象，这些目前还是走传统的渲染管线，如果这条路走了，可能也和现在的渲染管线遇到的问题一样了，这是一个难题。而且目前神经渲染实在是太慢了。nerf会不会在爬恐怖谷的时候也开始往下掉，这也是有可能的。</li><li>偷偷问了闫佬，原本定在今年的离线课程咕咕了，可能得明年了。</li><li>问了闫老师对实时全局光照的看法，目前科研界的研究可能还是太离线了，少有真正落地的项目，他也在考虑如何解决，可能要考虑做一个全新的算法出来，但没有展开太多（可能是后面排队要签名的人太多了。。。</li></ul><p><img src="/%E5%B0%B1%E5%9C%A8%E6%98%A8%E5%A4%A9%EF%BC%81%E9%97%AB%E4%BB%A4%E7%90%AA%E8%AE%BF%E9%97%AE%E4%BA%86%E6%88%91%E4%BB%AC%E7%BB%84%EF%BC%81%EF%BC%81%EF%BC%81/image-20230622004457254.png" alt="image-20230622004457254"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;闫令琪老师6.19报告的笔记和感想。&lt;/p&gt;
&lt;h2 id=&quot;Appearance&quot;&gt;&lt;a href=&quot;#Appearance&quot; class=&quot;headerlink&quot; title=&quot;Appearance&quot;&gt;&lt;/a&gt;&lt;strong&gt;Appearance&lt;/strong&gt;&lt;/h</summary>
      
    
    
    
    
    <category term="离线渲染" scheme="https://vymv.github.io/tags/%E7%A6%BB%E7%BA%BF%E6%B8%B2%E6%9F%93/"/>
    
    <category term="实时渲染" scheme="https://vymv.github.io/tags/%E5%AE%9E%E6%97%B6%E6%B8%B2%E6%9F%93/"/>
    
    <category term="前沿论文" scheme="https://vymv.github.io/tags/%E5%89%8D%E6%B2%BF%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>StableDiffusion初尝试和探索</title>
    <link href="https://vymv.github.io/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/"/>
    <id>https://vymv.github.io/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/</id>
    <published>2023-05-14T15:00:00.000Z</published>
    <updated>2023-07-23T09:59:23.171Z</updated>
    
    <content type="html"><![CDATA[<p>业余时间研究了一下SD，总结一下入门学到的东西吧。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>​分为基础模型和微调模型（lora等）。</p><h4 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h4><ul><li>基础模型通常非常大，以著名动漫模型anything为例，fp32的精度模型大小为4.27G，fp16的精度大小为2.13G（精度越高出图质量越好）。</li><li>有的基础模型擅长二次元创作，有的基础模型擅长真实感创作，可以根据需要切换。</li></ul><h4 id="Lora"><a href="#Lora" class="headerlink" title="Lora"></a>Lora</h4><p>​Lora是一种对大模型的微调模型，需要配合大模型一起使用，通常只有几百M。</p><p>​比如，GPT-3有1750亿参数，为了让它能干特定领域的活儿，需要做微调，但是如果直接对GPT-3做微调，成本太高太麻烦了。 </p><p>​Lora的做法是，冻结预训练好的模型权重参数，然后在每个Transformer块里注入可训练的层，由于不需要对模型的权重参数重新计算梯度，所以，大大减少了需要训练的计算量。 </p><p>​研究发现，Lora的微调质量与全模型微调相当。</p><p>​Lora通常由几百张或几千张特定风格的图片训练而成，你也可以训练自己的Lora。</p><h4 id="Vae"><a href="#Vae" class="headerlink" title="Vae"></a>Vae</h4><p>​可以视为模型的滤镜，如果生成的图片灰暗，饱和度很低，很可能是没有设置Vae，如这张图：</p><img src="/StableDiffusion初尝试和探索/image-20230505235253322.png" style="zoom:50%;" /><h3 id="模型下载使用"><a href="#模型下载使用" class="headerlink" title="模型下载使用"></a>模型下载使用</h3><h4 id="模型网站"><a href="#模型网站" class="headerlink" title="模型网站"></a>模型网站</h4><ul><li><p><a href="https://civitai.com/">civitai</a>（需要魔法，推荐）。</p></li><li><p><a href="https://huggingface.co/">huggingface</a>（综合模型网站，不只有AI绘画模型的网站）。</p></li></ul><h4 id="一些喜欢的模型和lora"><a href="#一些喜欢的模型和lora" class="headerlink" title="一些喜欢的模型和lora"></a>一些喜欢的模型和lora</h4><ul><li><p><a href="https://civitai.com/models/4855/anythingelse-v4">AnythingElse V4</a> （用来画二次元人物，不太会画手）。</p></li><li><p><a href="https://civitai.com/models/4468/counterfeit-v30">Counterfeit-V3.0</a> （喜欢用来画风景，搭配Lora：<a href="https://civitai.com/models/28511/ghiblistyleconceptsceneryv3">Ghibli_Style_Concept_scenery_V3</a>）。</p></li><li><p><a href="https://civitai.com/models/27259/tmnd-mix">TMND-Mix</a> （二次元风景和人物都很擅长）。</p></li><li><p><a href="https://civitai.com/models/16997?modelVersionId=20072">Standing Full Body with Background Style LoRA</a> （让二次元角色立绘拥有小背景）。</p></li></ul><h4 id="基础模型下载使用"><a href="#基础模型下载使用" class="headerlink" title="基础模型下载使用"></a>基础模型下载使用</h4><p>​以Anything为例：</p><p>​在<a href="https://huggingface.co/andite/anything-v4.0/tree/main">hugging face</a>打开可以看到很多选择，有.ckpt后缀的，也有.safetensor后缀。</p><p>​safetensor比ckpt更安全，加载时间更快。更具体的区别详见<a href="https://zhuanlan.zhihu.com/p/611248399">这里</a>。</p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/image-20230505231557638.png" alt="image-20230505231557638"></p><p>​打开<a href="https://civitai.com/models/4855/anythingelse-v4">C站</a>，右侧的信息中可以看到基础模型的type是checkpoint。</p><p>​Hash代表了该模型的id，如果是AI生成的图片，我们都能读取其prompt及参数信息以及模型hash，不同版本的同种模型hash也会不同。</p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/image-20230505232337468.png" alt="image-20230505232337468"></p><p>​C站下面有很多用户使用该模型生成的一些出色作品，并提供了prompt参考。</p><p>​下载好模型后放置在stable-diffusion-webui&#x2F;models&#x2F;Stable-diffusion下。</p><p>​点集右侧刷新，即可选择自己的模型。</p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/image-20230505233243426.png" alt="image-20230505233243426"></p><p>​切换大模型由于参数量巨大，通常需要等待几分钟的时间。</p><p>​一些大模型会提供官方的negative prompt，需要一并下载，例如<a href="https://civitai.com/models/4468/counterfeit-v30">这个</a>，下好后放置在stable-diffusion-webui&#x2F;embeddings目录下。</p><h4 id="lora下载使用"><a href="#lora下载使用" class="headerlink" title="lora下载使用"></a>lora下载使用</h4><p>​以<a href="https://civitai.com/models/16997/standing-full-body-with-background-style-lora">Standing Full Body with Background Style LoRA</a>为例，</p><p>​下载后放置在stable-diffusion-webui&#x2F;models&#x2F;Lora下，</p><p>​在右侧生成下面的第三个按钮可以选择Lora：</p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/image-20230505233615584.png" alt="image-20230505233615584"></p><p>​选择后，会被自动加入到prompt中，冒号后面可以调节lora权重，可以调用多个不同的lora并调节比例。</p><h4 id="vae下载使用"><a href="#vae下载使用" class="headerlink" title="vae下载使用"></a>vae下载使用</h4><p>​模型通常会有配套的vae。</p><p>​根据<a href="https://huggingface.co/andite/anything-v4.0/discussions/65">这个帖子</a>，下载适合Anything的vae：<a href="https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt">kl-f8-anime2.ckpt</a></p><p>​下好之后放置在stable-diffusion-webui&#x2F;models&#x2F;VAE下，</p><p>​在这里设置VAE：</p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/image-20230505234709733.png" alt="image-20230505234709733"></p><h4 id="作品生成"><a href="#作品生成" class="headerlink" title="作品生成"></a>作品生成</h4><ul><li><p>通常先生成低分辨率图像。</p></li><li><p>再通过放大，微调和修复细节。</p></li><li><p>设置好模型prompt及各种参数后，点击生成，就能在webui界面看到结果，output目录下也会保存每一张生成的图片。</p></li></ul><h3 id="功能及参数"><a href="#功能及参数" class="headerlink" title="功能及参数"></a>功能及参数</h3><h4 id="文生图"><a href="#文生图" class="headerlink" title="文生图"></a>文生图</h4><p>​prompt的权重和prompt的<strong>顺序</strong>有关，写在前面的prompt<strong>权重更高</strong>。</p><p>​<small style="opacity:0.5">（发现模型生成的是黄头发，想让它画黑头发，但在prompt末尾加了black hair，模型却还是不听我的。。。后来知道是权重太低了）</small></p><p>​使用括号：</p><ul><li><p>(prompt)，能将该词的权重乘1.1，如果双层括号，则乘两次1.1以此类推。</p></li><li><p>相反的中括号[]能将权重降低1.1倍。</p></li><li><p>也可以直接指定权重（black:1.4）</p><p>可根据出图反馈动态调整各词汇的权重。</p></li></ul><p>​通常prompt以这样的<strong>格式</strong>书写：</p><ul><li><p>第一段：画质 tag，画风 tag。</p></li><li><p>第二段：画面主体，主体强调，主体细节概括（主体可以是人、事、物、景）画面核心内容。</p></li><li><p>第三段：画面场景细节，或人物细节，embedding tag（Lora）。</p></li></ul><p><strong>常用的词汇：</strong></p><p>​<strong>风景图 prompt</strong></p><blockquote><p>​Landsape, (masterpiece, best quality:1.2), ultra detailed, cinematic lighting, beautiful lighting,lensflare,raytracing,HDR</p></blockquote><p>​<strong>二次元 girl prompt</strong></p><blockquote><p>​1girl,solo,masterpiece, best quality, looking_at_viewer,panorama,Beautiful Lighting,cinematiclighting,ray tracing,lensflare,tachi-e,full body, </p></blockquote><p>​<strong>negative prompt</strong></p><blockquote><p>​nsfw,(EasyNegative),(worst quality, low quality:1.4), (bad anatomy), (inaccurate limb:1.2),poorly eyes, extra digit,fewer digits,six fingers,(extra arms,extra legs:1.2),text,cropped,jpegartifacts,(signature), (watermark), username,blurry,more than five fingers in one palm,no thumb,no nails, title, multiple view, Reference sheet, curvy, plump, fat, muscular female, strabismus,</p></blockquote><p><strong>一些参数</strong></p><ul><li><strong>采样方法：</strong>一般都选择DPM++ 2M Karras。</li><li><strong>迭代步数（Steps）：</strong>顾名思义，迭代次数越高，生成的图细节越好，需要的时间也越长。</li><li><strong>提示词相关性（CFG）：</strong>一般在图生图中使用，文生图默认即可。</li><li><strong>随机种子：</strong>-1代表随机选择一个种子，也可以填写指定的种子，AI出图的随机性来源于此。</li></ul><h4 id="图生图"><a href="#图生图" class="headerlink" title="图生图"></a>图生图</h4><p><strong>一些参数</strong></p><ul><li><strong>提示词相关性（CFG）：</strong>顾名思义。</li><li><strong>重绘幅度（Denoising）：</strong>不清楚为什么“降噪”代表重绘幅度，通常如果是放大修复细节，重绘幅度设置为0.1~0.2，如果普通图生图则0.5~0.9都有。</li></ul><p>​如下图，左侧为原图，中间重绘幅度为，还能清楚地看到画卷展开，右侧重绘幅度为，已经看不到画卷了。</p><div style="display:flex; justify-content:center; align-items:center;">  <img src="/StableDiffusion初尝试和探索/279.png" >  <img src="/StableDiffusion初尝试和探索/00010-53652654.png" >  <img src="/StableDiffusion初尝试和探索/00002-604226150.png" ></div><h4 id="放大"><a href="#放大" class="headerlink" title="放大"></a>放大</h4><p>​可以是纯放大。</p><p>​也可以用来修复细节，给予一定的重绘幅度，画面有时会出现一些较为明显的向好改变。</p><p>​例如下面这张，除了手部，其他部分都不错。</p><img src="/StableDiffusion初尝试和探索/00058-452352455.png" alt="00058-452352455"  /><p>​放大后：</p><img src="/StableDiffusion初尝试和探索/00004-1929881450.png" alt="00004-1929881450" style="zoom:50%;" /><p>​因为negative prompt中给bad finger加了很高的权重，模型倾向于不画右手了。。。</p><p>​后来调了种子又出了一张还不错的：</p><img src="/StableDiffusion初尝试和探索/1.png" alt="1" style="zoom: 33%;" /><h4 id="反推prompt"><a href="#反推prompt" class="headerlink" title="反推prompt"></a>反推prompt</h4><p>​试了试CLIP反推，感觉很多细节没有描写到位。</p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/326.webp" alt="326"></p><p>​SD给出的prompt是：</p><blockquote><p>​a woman dressed in a white and gold outfit with a cat on her lap and a clock on her arm, Fan Qi, official art, a character portrait, ukiyo-e</p></blockquote><p>​但显然不是cat而是fox，手摇铃被描述为clock，可能是训练的时候没见过吧。</p><p>​用上面的prompt再生成一张图试试：</p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/QQ%E5%9B%BE%E7%89%8720230515000205.jpg" alt="326"></p><h4 id="图片信息"><a href="#图片信息" class="headerlink" title="图片信息"></a>图片信息</h4><p>​顾名思义，能读到AI作图的prompt和参数。可以用来学习网络上各种优秀作品的prompt。</p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/image-20230506000831230.png" alt="image-20230506000831230"></p><h4 id="种子微调"><a href="#种子微调" class="headerlink" title="种子微调"></a>种子微调</h4><img src="/StableDiffusion初尝试和探索/QQ图片20230515000503.jpg" alt="QQ图片20230515000503" style="zoom:50%;" /><p>​生成了一张不错的图，想多生成一些类似的，挑一张更好的。</p><ul><li>在随机种子这里打钩</li></ul><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/image-20230515000707407.png" alt="image-20230515000707407"></p><p>​随机种子和差异随机种子都填写这张图片的Seed</p><ul><li><p>调整差异强度，例如0.18</p></li><li><p>调整每批数量，例如4，点击生成</p></li></ul><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/$VH@RU86XBX_G9%60ZQZ1HQ5R.jpg" alt="img"></p><h4 id="修复手指"><a href="#修复手指" class="headerlink" title="修复手指"></a>修复手指</h4><p>​画不好手指是AI模型的通病了，通常有以下几种解决方法：</p><ul><li><p>inpaint（局部重绘）</p><p>通过在异样手指处手绘蒙版，修复手指。</p></li></ul><div style="display:flex;">  <img src="/StableDiffusion初尝试和探索/00000-3058949938.png" >  <img src="/StableDiffusion初尝试和探索/00007-3058949938.png" ></div><ul><li><p>放大</p><p>（见上文）</p></li><li><p>control net（TODO）</p></li><li><p>Lora（TODO）</p></li></ul><h4 id="一些生僻参数"><a href="#一些生僻参数" class="headerlink" title="一些生僻参数"></a>一些生僻参数</h4><p>​经常在C站的图片信息里看到的生僻参数的解释。</p><ul><li><strong>ENSD</strong>：默认建议值31337，详见<a href="https://www.reddit.com/r/StableDiffusion/comments/10d093y/ensd_what_is_it/">这里</a>。</li><li><strong>Clip skip：</strong>默认2，详见<a href="https://zhuanlan.zhihu.com/p/621276242">这里</a>。</li></ul><h3 id="一些自己生成的好看的图"><a href="#一些自己生成的好看的图" class="headerlink" title="一些自己生成的好看的图"></a>一些自己生成的好看的图</h3><img src="/StableDiffusion初尝试和探索/00175-654345.png" alt="00175-654345" style="zoom:100%;" /><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00003-24352435.png" alt="00003-24352435"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00062-365645534.png" alt="00062-365645534"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00102-2768684264.png" alt="00102-2768684264"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00051-653425.png" alt="00051-653425"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00241-470777179.png" alt="00241-470777179"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00168-3250371279.png" alt="00168-3250371279"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00325-3193787633-1683470099936.png" alt="00325-3193787633"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00020-264466535.png" alt="00020-264466535"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00313-2789391629.png" alt="00313-2789391629"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00363-654364532543.png" alt="00363-654364532543"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00233-4038965757.png" alt="00233-4038965757"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00305-2867403363.png" alt="00305-2867403363"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00323-3207705890.png" alt="00323-3207705890"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00018-6546345.png" alt="00018-6546345"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00296-1539898891.png" alt="00296-1539898891"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00252-2482152167.png" alt="00252-2482152167"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00333-541990144.png" alt="00333-541990144"></p><p><img src="/StableDiffusion%E5%88%9D%E5%B0%9D%E8%AF%95%E5%92%8C%E6%8E%A2%E7%B4%A2/00002-43589576.png" alt="00002-43589576"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;业余时间研究了一下SD，总结一下入门学到的东西吧。&lt;/p&gt;
&lt;h3 id=&quot;模型&quot;&gt;&lt;a href=&quot;#模型&quot; class=&quot;headerlink&quot; title=&quot;模型&quot;&gt;&lt;/a&gt;模型&lt;/h3&gt;&lt;p&gt;​	分为基础模型和微调模型（lora等）。&lt;/p&gt;
&lt;h4 id=&quot;基础模</summary>
      
    
    
    
    <category term="AI绘画" scheme="https://vymv.github.io/categories/AI%E7%BB%98%E7%94%BB/"/>
    
    
    <category term="AI绘画" scheme="https://vymv.github.io/tags/AI%E7%BB%98%E7%94%BB/"/>
    
  </entry>
  
  <entry>
    <title>StableDiffusion本地部署记录</title>
    <link href="https://vymv.github.io/StableDiffusion%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/"/>
    <id>https://vymv.github.io/StableDiffusion%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/</id>
    <published>2023-04-25T09:15:54.000Z</published>
    <updated>2023-07-23T09:59:23.306Z</updated>
    
    <content type="html"><![CDATA[<p>昨晚刷b站时，看到这个视频</p><p>【如果985大学是二次元手游，你想要抽哪张？】 <a href="https://www.bilibili.com/video/BV1Kv4y1E7rA/?share_source=copy_web&vd_source=fd91412d6ab5fedb341301b55cce0c61">https://www.bilibili.com/video/BV1Kv4y1E7rA/?share_source=copy_web&amp;vd_source=fd91412d6ab5fedb341301b55cce0c61</a></p><p><img src="/StableDiffusion%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/image-20230425170141364.png" alt="image-20230425170141364"></p><p>超级喜欢这些二次元小公主！激动之下我也要来画！</p><h1 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a><strong>创建虚拟环境</strong></h1><p>conda create -n sd2 python&#x3D;3.10.6</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd D:\Code\stable-diffusion-webui</span><br><span class="line">webui.bat</span><br></pre></td></tr></table></figure><p>这条命令一定会报错，执行的目的只是copy虚拟环境到stable-diffusion-webui目录</p><p>进入stable-diffusion-webui，已经可以看到venv\Scripts\Python.exe了</p><p>进入Scripts文件夹，执行activate激活虚拟环境</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python --version</span><br></pre></td></tr></table></figure><p>确保是3.10.6</p><h1 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h1><p>requirements.txt注释掉以下包</p><p><img src="/StableDiffusion%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/v2-cfda75f29b463c4717a9a3529660e10e_720w-16824117117173.webp" alt="img"></p><p>确保虚拟环境激活的情况下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install gradio</span><br></pre></td></tr></table></figure><p>打开launch.py，22行改为True</p><p><img src="/StableDiffusion%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/v2-355fc6848083dd39efa578d5aae38595_720w.webp" alt="img"></p><p>221行的命令通过手动执行完成</p><p><img src="/StableDiffusion%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/v2-2f408f167515d102de9c699134b083d6_720w.webp" alt="img"></p><p>（由于这里容易出现网络错误：这里使用ssh的方式clone仓库，然后手动在本地安装，确保虚拟环境正确，关闭魔法）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117</span><br><span class="line">pip install -r requirements_versions.txt</span><br><span class="line">pip install xformers==0.0.16rc425</span><br><span class="line"></span><br><span class="line">cd D:\Code\stable-diffusion-webui\repositories</span><br><span class="line"></span><br><span class="line">git clone git@github.com:TencentARC/GFPGAN.git</span><br><span class="line">git checkout 8d2447a2d918f8eba5a4a01463fd48e45126a379</span><br><span class="line">cd GFPGAN</span><br><span class="line">pip install .</span><br><span class="line"></span><br><span class="line">git clone git@github.com:openai/CLIP.git</span><br><span class="line">git checkout d50d76daa670286dd6cacf3bcd80b5e4823fc8e1</span><br><span class="line">cd CLIP</span><br><span class="line">pip install .</span><br><span class="line"></span><br><span class="line">git clone git@github.com:mlfoundations/open_clip.git</span><br><span class="line">git checkout bb6e834e9c70d9c27d0dc3ecedeebeaeb1ffad6b</span><br><span class="line">cd open_clip</span><br><span class="line">pip install .</span><br><span class="line"></span><br><span class="line">git clone git@github.com:Stability-AI/stablediffusion.git</span><br><span class="line">git checkout cf1d67a6fd5ea1aa600c4df58e5b47da45f6bdbf</span><br><span class="line">cd stablediffusion</span><br><span class="line">pip install .</span><br><span class="line"></span><br><span class="line">git clone git@github.com:CompVis/taming-transformers.git</span><br><span class="line">git checkout 24268930bf1dce879235a7fddd0b2355b84d7ea6</span><br><span class="line">cd taming-transformers</span><br><span class="line">pip install .</span><br><span class="line"></span><br><span class="line">git clone git@github.com:crowsonkb/k-diffusion.git</span><br><span class="line">git checkout 5b3af030dd83e0297272d861c19477735d0317ec</span><br><span class="line">cd k-diffusion</span><br><span class="line">pip install .</span><br><span class="line"></span><br><span class="line">git clone git@github.com:sczhou/CodeFormer.git</span><br><span class="line">git checkout c5b4593074ba6214284d6acd5f1719b6c5d739af</span><br><span class="line">cd CodeFormer</span><br><span class="line">pip install -r requirement.txt</span><br><span class="line"></span><br><span class="line">git clone  git@github.com:salesforce/BLIP.git</span><br><span class="line">git checkout 48211a1594f1321b00f14c9f7a5b4813144b2fb9</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>环境安装完毕，但这里直接launch会报错</p><p>见issue:<a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/8769">https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/8769</a></p><h1 id="下载模型"><a href="#下载模型" class="headerlink" title="下载模型"></a>下载模型</h1><p>下载sd-v1-4.ckpt，放置在…stable-diffusion-webui\models\Stable-diffusion目录下</p><p><a href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/blob/main/sd-v1-4.ckpt">https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/blob/main/sd-v1-4.ckpt</a></p><p>重命名为 Model.ckpt<br>进入…stable-diffusion-webui修改webui-user.bat为 :<br>…<br>set COMMANDLINE_ARGS&#x3D;<br>git pull<br>call webui.bat</p><h1 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h1><p>运行webui-user.bat</p><p><img src="/StableDiffusion%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/image-20230425165257626.png" alt="image-20230425165257626"></p><p>出现本地链接即为成功</p><p><img src="/StableDiffusion%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/image-20230425165411002.png" alt="image-20230425165411002"></p><p>完成以上就可以美美画图啦！</p><p>参考：<a href="https://zhuanlan.zhihu.com/p/618163310">https://zhuanlan.zhihu.com/p/618163310</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;昨晚刷b站时，看到这个视频&lt;/p&gt;
&lt;p&gt;【如果985大学是二次元手游，你想要抽哪张？】 &lt;a href=&quot;https://www.bilibili.com/video/BV1Kv4y1E7rA/?share_source=copy_web&amp;vd_source=fd9141</summary>
      
    
    
    
    <category term="AI绘画" scheme="https://vymv.github.io/categories/AI%E7%BB%98%E7%94%BB/"/>
    
    
    <category term="AI绘画" scheme="https://vymv.github.io/tags/AI%E7%BB%98%E7%94%BB/"/>
    
  </entry>
  
  <entry>
    <title>hexo建站记录</title>
    <link href="https://vymv.github.io/hexo%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95/"/>
    <id>https://vymv.github.io/hexo%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95/</id>
    <published>2023-03-25T09:25:17.000Z</published>
    <updated>2023-07-23T09:59:23.319Z</updated>
    
    <content type="html"><![CDATA[<h2 id="建站"><a href="#建站" class="headerlink" title="建站"></a>建站</h2><p><strong>参考：</strong></p><p>hexo超完整的搭建教程，让你拥有一个专属个人博客 - 直上云霄的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/44213627">https://zhuanlan.zhihu.com/p/44213627</a></p><p>注意deploy branch是main</p><h2 id="修改主题"><a href="#修改主题" class="headerlink" title="修改主题"></a>修改主题</h2><p>进入你的 hexo 博客的工作路径</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone -b master https://github.com/Molunerfinn/hexo-theme-melody themes/melody</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/u010416101/article/details/102986439">Hexo博客搭建之主题构建(melody)_melody主题_在风中的意志的博客-CSDN博客</a></p><p>拷贝一个_config.yml到_config.melody.yml</p><h2 id="每次修改后"><a href="#每次修改后" class="headerlink" title="每次修改后"></a>每次修改后</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo g  //generate 生成静态页面</span><br><span class="line">hexo s  //启动server</span><br><span class="line">hexo d // deploy 部署到github</span><br></pre></td></tr></table></figure><h2 id="个性化"><a href="#个性化" class="headerlink" title="个性化"></a>个性化</h2><p>配置_config.melody.yml，参考<a href="https://blog.csdn.net/u010416101/article/details/103198952/">https://blog.csdn.net/u010416101/article/details/103198952/</a></p><h3 id="头像，顶部图片"><a href="#头像，顶部图片" class="headerlink" title="头像，顶部图片"></a>头像，顶部图片</h3><p>放置在项目的images目录下，可使用相对路径引用</p><p><a href="https://blog.csdn.net/Dra_Tammer/article/details/126260499">Hexo-显示用户头像_hexo 头像url_没牙的驯龙师的博客-CSDN博客</a></p><h3 id="Menu"><a href="#Menu" class="headerlink" title="Menu"></a>Menu</h3><ol><li>前往你的Hexo博客的根目录</li><li>输入<code>hexo new page tags</code> &#x2F;&#x2F; hexo new page category</li><li>你会找到<code>source/tags/index.md</code>这个文件</li><li>修改这个文件：</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 标签</span><br><span class="line">date: 2018-01-05 00:00:00</span><br><span class="line">type: &quot;tags&quot; // category则填&quot;category&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>然后配置<code>melody.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">menu:Home:</span> <span class="string">/</span></span><br><span class="line">  <span class="attr">Archives:</span> <span class="string">/archives</span></span><br><span class="line">  <span class="attr">Tags:</span> <span class="string">/tags</span></span><br><span class="line">  <span class="attr">Categories:</span> <span class="string">/categories</span></span><br></pre></td></tr></table></figure><h3 id="样式调整"><a href="#样式调整" class="headerlink" title="样式调整"></a>样式调整</h3><p>参考：</p><p><a href="https://blog.yychi.website/blog_archive/2018/12/Melody-%E4%B8%BB%E9%A2%98%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E5%AD%97%E4%BD%93/#%E5%8E%BB%E9%99%A4%E5%88%97%E8%A1%A8%E6%A0%B7%E5%BC%8F">https://blog.yychi.website/blog_archive/2018/12/Melody-%E4%B8%BB%E9%A2%98%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E5%AD%97%E4%BD%93/#去除列表样式</a></p><p>不是很喜欢图片下面显示图片名</p><p>像这样，很尴尬：</p><p><img src="/hexo%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95/16797364098181.png" alt="img"></p><p>在themes\melody\source\js\fancybox.js中注释掉以下即可：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// if (alt) &#123;</span></span><br><span class="line"><span class="comment">//   $wrap.after(&#x27;&lt;div class=&quot;img-alt&quot;&gt;&#x27; + alt + &#x27;&lt;/div&gt;&#x27;)</span></span><br><span class="line"><span class="comment">// &#125;</span></span><br></pre></td></tr></table></figure><h2 id="写文章"><a href="#写文章" class="headerlink" title="写文章"></a>写文章</h2><h3 id="文章图片插入"><a href="#文章图片插入" class="headerlink" title="文章图片插入"></a>文章图片插入</h3><p><a href="https://zhuanlan.zhihu.com/p/265077468">https://zhuanlan.zhihu.com/p/265077468</a></p><p>注意需要将permalink修改成：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">permalink: :title/</span><br></pre></td></tr></table></figure><h3 id="给文章添加Tag，Category"><a href="#给文章添加Tag，Category" class="headerlink" title="给文章添加Tag，Category"></a>给文章添加Tag，Category</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tags:</span><br><span class="line">- 标签1</span><br><span class="line">- 标签2</span><br></pre></td></tr></table></figure><p>Category同理</p><p><a href="https://blog.csdn.net/nineya_com/article/details/103316683">Hexo博客发表文章、草稿、添加分类和标签_hexo发布文章_玖涯的博客-CSDN博客</a></p><h3 id="YAMLException-can-not-read-a-block-mapping-entry；-a-multiline-key-may-not-be-an-implicit-key-4-1"><a href="#YAMLException-can-not-read-a-block-mapping-entry；-a-multiline-key-may-not-be-an-implicit-key-4-1" class="headerlink" title="YAMLException: can not read a block mapping entry； a multiline key may not be an implicit key (4:1)"></a>YAMLException: can not read a block mapping entry； a multiline key may not be an implicit key (4:1)</h3><p>冒号后面需要空格</p><p><a href="https://blog.csdn.net/qq_53361975/article/details/126791819">YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key (4:1)_</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;建站&quot;&gt;&lt;a href=&quot;#建站&quot; class=&quot;headerlink&quot; title=&quot;建站&quot;&gt;&lt;/a&gt;建站&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;参考：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;hexo超完整的搭建教程，让你拥有一个专属个人博客 - 直上云霄的文章 - 知乎 &lt;</summary>
      
    
    
    
    <category term="其他" scheme="https://vymv.github.io/categories/%E5%85%B6%E4%BB%96/"/>
    
    
    <category term="其他" scheme="https://vymv.github.io/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title>Surfel GI阅读笔记</title>
    <link href="https://vymv.github.io/GI%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>https://vymv.github.io/GI%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2023-03-25T08:49:01.000Z</published>
    <updated>2023-07-23T09:59:23.164Z</updated>
    
    <content type="html"><![CDATA[<p><strong>原文链接：</strong></p><p><a href="https://advances.realtimerendering.com/s2021/SIGGRAPH%20Advances%202021%20-%20Surfel%20GI.pdf">https://advances.realtimerendering.com/s2021/SIGGRAPH%20Advances%202021%20-%20Surfel%20GI.pdf</a></p><h3 id="Surfel存储"><a href="#Surfel存储" class="headerlink" title="Surfel存储"></a>Surfel存储</h3><ul><li>transform ID（贴在哪个mesh上）：可由 G-buffer（Visibility Buffer）获取 ID</li><li>local position（相对于 mesh 的位置）：可由 G-buffer 获取 depth，推出 world position 再乘 transform 的逆变换得到</li><li>local normal（相对于 mesh 的朝向）：可由 G-buffer 获取 world normal 再乘 transform 的逆变换得到</li><li>irradiance（存储累积探测后的 irradiance）：通过 tracing 探测光照并累积起来得到的 irradiance</li><li>depth buffer（存储半球方向上的最小深度）：用于判断 visibility</li></ul><h3 id="Surfel构建"><a href="#Surfel构建" class="headerlink" title="Surfel构建"></a>Surfel构建</h3><ul><li><strong>在哪里派生</strong>：屏幕空间划分成Tile，一个Tile中的Surfel太少了就多加点。</li><li><strong>位置如何移动</strong>：跟随attach的表面</li><li><strong>大小如何变动</strong>：每个Surfel在屏幕上的投影大小始终保持恒定，意味着，当镜头向Surfel推进，Surfel会自己缩小以保证投影在屏幕的面积恒定。</li><li><strong>回收机制</strong>：三个因素：1. 场景中有多少Surfel。2. 最近一次贡献光照的时间。3. Surfel有多远。</li></ul><h3 id="加速结构"><a href="#加速结构" class="headerlink" title="加速结构"></a><strong>加速结构</strong></h3><ul><li>需要快速找到一个着色点附近有哪些Surfels</li><li>every frame，将surfel注入Grid，如果跨格子则同时注入两个格子。</li><li>保证每个格子的大小比Surfel的半径大。</li><li>远处的Surfel很大，因此格子也需要很大。于是有了如下结构：</li></ul><p><img src="/GI%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/20ec5dc6-2035-464d-b282-94e84e519204.png" alt="20ec5dc6-2035-464d-b282-94e84e519204"></p><h3 id="着色漏光处理"><a href="#着色漏光处理" class="headerlink" title="着色漏光处理"></a>着色漏光处理</h3><p>切比雪夫，同DDGI</p><h3 id="Surfel发出光线"><a href="#Surfel发出光线" class="headerlink" title="Surfel发出光线"></a>Surfel发出光线</h3><p>If a surfel is seeing high variance, it requests more rays in order to converge faster. In contrast, if a surfel sees low variance, it can send much fewer rays.  </p><h3 id="Surfel-irradiance计算的重要性采样"><a href="#Surfel-irradiance计算的重要性采样" class="headerlink" title="Surfel irradiance计算的重要性采样"></a>Surfel irradiance计算的重要性采样</h3><p><img src="/GI%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3421b416-6c0f-4c33-be25-771bf58ccaba.png" alt="3421b416-6c0f-4c33-be25-771bf58ccaba"></p><p>这两个Surfels的颜色差距很大，因为一个采样到了一根有效光线，一个采样了两根。</p><p>这样容易导致很高的noise</p><p><img src="/GI%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/4f4db7e3-7284-4d2a-bb6f-505022e238cd.png" alt="4f4db7e3-7284-4d2a-bb6f-505022e238cd"></p><p>（类似于MIS中采样光源还是采样brdf，当表面十分粗糙时，较小的光源容易产生较大的方差，也就是上图的noise）</p><p><img src="/GI%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20230403163046768.png" alt="image-20230403163046768"></p><p>四叉树的方案，每个leaf node的radiance相同，direction按Radiance成比例</p><p><img src="/GI%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20230403163136695.png" alt="image-20230403163136695"></p><h3 id="Irradiance-Sharing"><a href="#Irradiance-Sharing" class="headerlink" title="Irradiance Sharing"></a>Irradiance Sharing</h3><p>允许surfel查询neighbour的radiance，以减少noise</p><h3 id="Ray-Sorting"><a href="#Ray-Sorting" class="headerlink" title="Ray Sorting"></a>Ray Sorting</h3><p>Ray binning，把相近位置和方向的光线分组在一个Bin里，加速计算</p><h3 id="Many-Light"><a href="#Many-Light" class="headerlink" title="Many Light"></a>Many Light</h3><p>根据概率随机采样一些光线</p><p>Stochastic Lightcuts</p><ul><li>根据概率选择一部分Light采样</li></ul><p>Reservoir Sampling</p><ul><li>很火的ReSTIR</li></ul><h3 id="Transparency"><a href="#Transparency" class="headerlink" title="Transparency"></a>Transparency</h3><ul><li>透明物体没法用surfel</li><li>改用probe</li><li>probe用clipmap组织</li><li>与Gbuffer无关</li></ul><p><strong>参考</strong>：</p><p><a href="https://www.cnblogs.com/KillerAery/p/16748160.html#ray-generation--msme">https://www.cnblogs.com/KillerAery/p/16748160.html#ray-generation--msme</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;原文链接：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://advances.realtimerendering.com/s2021/SIGGRAPH%20Advances%202021%20-%20Surfel%20GI.pdf&quot;&gt;htt</summary>
      
    
    
    
    
    <category term="Screen Probe" scheme="https://vymv.github.io/tags/Screen-Probe/"/>
    
    <category term="Rendering" scheme="https://vymv.github.io/tags/Rendering/"/>
    
  </entry>
  
</feed>
